{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from math import log, exp, sqrt\n",
    "import pickle\n",
    "import sys\n",
    "import random\n",
    "import math\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(sys.argv) != 3:\n",
    "    print (\"Usage: pypy Online.py <epochs> <use_example_probability>\")\n",
    "    print (\"epochs is number of passes over the training data\")\n",
    "    print (\"use_example_probability is the probability of using an example in an epoch\")\n",
    "    sys.exit(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = int(sys.argv[1])\n",
    "print (\"Number of Epochs\", epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the cheap way to add randomness to the order of training example\n",
    "# but use with caution as it does not guarantee all training examples will be seen\n",
    "# Use 1 if you want to train in order examples appear in the file\n",
    "use_example_probability = float(sys.argv[2])\n",
    "print \"Use Example Probability\", use_example_probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = 'trainPredictors.csv' #path to training file\n",
    "label = 'trainLabels.csv' #path to label file of training data\n",
    "test = 'TestData2.csv' #path to testing file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "range(0, 17)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Intercept = 0\n",
    "# Object_Description = 1\n",
    "# Text_2 = 2\n",
    "# SubFund_Description = 3\n",
    "# Job_Title_Description = 4\n",
    "# Text_3 = 5\n",
    "# Text_4 = 6\n",
    "# Sub_Object_Description = 7\n",
    "# Location_Description = 8\n",
    "# FTE = 9\n",
    "# Function_Description = 10\n",
    "# Facility_or_Department = 11\n",
    "# Position_Extra = 12\n",
    "# Total = 13\n",
    "# Program_Description = 14\n",
    "# Fund_Description = 15\n",
    "# Text_1 = 16\n",
    "\n",
    "originals = range(17)\n",
    "# Found that removing 5 (Text_3) and 7 (Sub Object Description) generaly helped\n",
    "originals.remove(5)\n",
    "originals.remove(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pairs [[1, 2, 3, 4], [6, 8], [4, 12], [1, 4, 8, 10]]\n",
      "triples [[1, 4, 12]]\n"
     ]
    }
   ],
   "source": [
    "#interaction paris and triples\n",
    "pairs =[[1,2,3,4],[6,8],[4,12],[1,4,8,10]]\n",
    "triples = [[1,4,12]]\n",
    "print ('pairs', pairs)\n",
    "print ('triples', triples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "262144"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "D = 2 **18 # number of weights use for each model, we have 104 of them\n",
    "alpha = .10  # learning rate for sgd optimization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utilities ############################################\n",
    "# Used for assigning the number feat to a categorical level 0 to N\n",
    "# INPUT:\n",
    "#     feat: the numerical predictor\n",
    "#     b: list representing the boundaries for bins\n",
    "# OUTPUT:\n",
    "#     a categorical level 0 to N\n",
    "\n",
    "def boundary(feat,b):\n",
    "    f = float(feat)\n",
    "    s = 0\n",
    "    for step in b:\n",
    "        if f < step\n",
    "        s += 1\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#our hashing function\n",
    "#input: s: the string or number\n",
    "#output: an intiger between 0 and D-1\n",
    "def hash_it(s):\n",
    "    return abs(hash(s)) % D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function, generator definitions ############################################\n",
    "\n",
    "# A. x, y generator\n",
    "# This is where:\n",
    "# * All the feature hashes are generated\n",
    "# * All feature engineering happens\n",
    "# INPUT:\n",
    "#     path: path to TrainPredictors.csv or TestData2.csv\n",
    "#     label_path: (optional) path to TrainLabels.csv\n",
    "# YIELDS:\n",
    "#     ID: id of the instance \n",
    "#     x: list of hashes for predictors\n",
    "#     y: (if label_path is present) binary label\n",
    "\n",
    "def data(path, label_path=None):\n",
    "    # Boundaries for numerical binning of FTE (9) and Total (13)\n",
    "    b13 =[\n",
    "    -706.968,-8.879,\n",
    "    7.85,41.972,\n",
    "    73.798,109.55,\n",
    "    160.786,219.736,\n",
    "    318.619,461.23,\n",
    "    646.73,938.36,\n",
    "    1317.584,2132.933,\n",
    "    3652.662,6659.524,\n",
    "    18551.459,39754.287,\n",
    "    64813.342,129700000\n",
    "    ]\n",
    "    \n",
    "    b9 = [0.0,0.00431,0.131,0.911,1,50]\n",
    "    \n",
    "    for t, line in enumerate(open(path)):\n",
    "        #intercept term\n",
    "        x = [0]\n",
    "        \n",
    "        #skip headers\n",
    "        if t == 0:\n",
    "            if label_path:\n",
    "                label = open(label_path)\n",
    "                label.readline() #we don't need the headers\n",
    "            continue\n",
    "        # c is an index for the kept original features (15 of them)\n",
    "        # TODO: drop c and use m for hashing, c was kept for reproducibility\n",
    "        # m is the index for all the original features (17 of them)\n",
    "        # feat is the original raw text or value for feature\n",
    "        c = 0\n",
    "        for m, feat in enumerate(line.rstrip().split(',')):\n",
    "            #drop unwanted original features\n",
    "            if m not in originals:\n",
    "                continue\n",
    "                \n",
    "            if m == 0:\n",
    "                ID = int(feat)\n",
    "            else:\n",
    "                #convert floats into catogorical levels\n",
    "                #variables 9 (FTE) nad 13 (Total) are only numericals\n",
    "                if m == 13:\n",
    "                    if feat == \"\": feat = 0\n",
    "                    feat = boundary(feat,b13)\n",
    "                if m == 19:\n",
    "                    if feat == \"\": feat = 0\n",
    "                    feat = boundary(feat,b9)\n",
    "                #convert floats into catogorical levels\n",
    "                #variables 9 (FTE) nad 13 (Total) are only numericals\n",
    "            # Lowercase and trim so hashes match more often\n",
    "            feat = str(feat).strip().lowercase()\n",
    "\n",
    "            # First we hash the original feature.  For example, if the\n",
    "            # feature is \"special education\" and the original feature index is 4, we \n",
    "            # hash \"4_special education\"\n",
    "            \n",
    "            original_feature = str(c) + '_' + feat\n",
    "            x.append(hash_it(original_feature))\n",
    "            \n",
    "            #we break up the origianl feature into the word parts\n",
    "            #create bag-of-words features here\n",
    "            parts = re.split(' |/|-',feat)\n",
    "            \n",
    "            for i in range(len(parts)):\n",
    "                token = parts[i].strip.lower()\n",
    "                if token == '':continue\n",
    "                # First we hash each token along with its original position index\n",
    "                # For example, for the feature value \"special education\" we hash \n",
    "                # its tokens as \"4_special\" and \"4_education\" in successive steps of this loop\n",
    "                \n",
    "                positioned_word = str(c) + '_' + token\n",
    "                x.append( hash_it(positioned_word))\n",
    "                \n",
    "                # Next we hash each token by itself, ignoring any information about its position\n",
    "                # For example, for \"special education\" we hash \"special\" and \"education\"\n",
    "                # regardless of what index position the original feature appeared in.  \n",
    "                # This views all the feature values in an example as making up a single document\n",
    "                \n",
    "                x.append( hash_it( token))\n",
    "            c = c + 1\n",
    "            \n",
    "        # Up to this point we've been breaking original features down into smaller features\n",
    "        # Now we level up and compose original features with each other into larger interction features \n",
    "        \n",
    "        row = line.rstrip().split(',')\n",
    "        # Start with pairs.  Make pairs from interaction groups defined in pairs variable.\n",
    "        for interactions in pairs:\n",
    "            for i in xrange(len(interactions)):\n",
    "                for j in xrange(i+1,len(interactions)):\n",
    "                    pair = row[interactions[i]]+\"_x_\"+row[interactions[j]]\n",
    "                    x.append( hash_it(pair) )\n",
    "\n",
    "        # Do the same thing for triples\n",
    "        for triple in triples:\n",
    "            trip = row[triple[0]]+\"_x_\"+row[triple[1]] + '_x_' +row[triple[2]]\n",
    "            x.append( hash_it(trip) )\n",
    "            \n",
    "        if label_path:\n",
    "            y = [float(y) for y in label.readline().split(',')[1:]]\n",
    "\n",
    "        yield (ID, x, y) if label_path else (ID, x)\n",
    "        \n",
    "B. Bounded logloss\n",
    "# INPUT:\n",
    "#     p: our prediction\n",
    "#     y: real answer\n",
    "# OUTPUT\n",
    "#     bounded logarithmic loss of p given y\n",
    "def logloss(p, y):\n",
    "    p = max(min(p, 1. - 10e-15), 10e-15)\n",
    "    return -log(p) if y == 1. else -log(1. - p)\n",
    "\n",
    "\n",
    "# C. Get probability estimation on x\n",
    "# INPUT:\n",
    "#     x: features\n",
    "#     w: weights\n",
    "# OUTPUT:\n",
    "#     probability of p(y = 1 | x; w)\n",
    "def predict(x, w):\n",
    "    wTx = 0.\n",
    "    for i in x:  # do wTx\n",
    "        wTx += w[i] * 1.  # w[i] * x[i], but if i in x we got x[i] = 1.\n",
    "    return 1. / (1. + exp(-max(min(wTx, 100.), -100.)))  # bounded sigmoid\n",
    "                    \n",
    "# D. Update given model\n",
    "# INPUT:\n",
    "# alpha: learning rate\n",
    "#     w: weights\n",
    "#     n: sum of previous absolute gradients for a given feature\n",
    "#        this is used for adaptive learning rate\n",
    "#     x: feature, a list of indices\n",
    "#     p: prediction of our model\n",
    "#     y: answer\n",
    "# MODIFIES:\n",
    "#     w: weights\n",
    "#     n: sum of past absolute gradients\n",
    "def update(alpha, w, n, x, p, y,k):\n",
    "    for i in x:\n",
    "        # alpha / sqrt(n) is the adaptive learning rate\n",
    "        # (p - y) * x[i] is the current gradient\n",
    "        # note that in our case, if i in x then x[i] = 1.\n",
    "        n[i] += abs(p - y)\n",
    "        w[i] = w[i] - ((p - y) * 1. ) * alpha / n[i] ** 0.5 \n",
    "                \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training and testing #######################################################\n",
    "start = datetime.now()\n",
    "# Number of models.  \n",
    "DIM = 104\n",
    "K = range(DIM)\n",
    "w = [[0.] * D for k in range(DIM)]\n",
    "n = [[0.] * D for k in range(DIM)]\n",
    "\n",
    "random.seed(1234)\n",
    "\n",
    "loss = 0.\n",
    "rec = 0\n",
    "\n",
    "for i in range(epochs):\n",
    "    for ID, x, y in data(train,label):\n",
    "        # Randomly choose whether or not to train with this example in this epoch\n",
    "        if random.random() > use_example_probability: continue\n",
    "        # record counter\n",
    "        rec += 1\n",
    "        # get predictions and train on all labels\n",
    "        for k in K:\n",
    "            p = predict(x, w[k])\n",
    "            update(alpha, w[k], n[k], x, p, y[k],k)\n",
    "            loss += logloss(p, y[k])  # for progressive validation\n",
    " \n",
    "        # print out progress, so that we know everything is working\n",
    "        if rec % 50000 == 0:\n",
    "            print('%s\\tencountered: %d\\tcurrent logloss: %f' % (\n",
    "                datetime.now(), rec, (loss/float(DIM))/rec))\n",
    "\n",
    "h = ',Function__Aides Compensation,Function__Career & Academic Counseling,Function__Communications,Function__Curriculum Development,Function__Data Processing & Information Services,Function__Development & Fundraising,Function__Enrichment,Function__Extended Time & Tutoring,Function__Facilities & Maintenance,Function__Facilities Planning,\"Function__Finance, Budget, Purchasing & Distribution\",Function__Food Services,Function__Governance,Function__Human Resources,Function__Instructional Materials & Supplies,Function__Insurance,Function__Legal,Function__Library & Media,Function__NO_LABEL,Function__Other Compensation,Function__Other Non-Compensation,Function__Parent & Community Relations,Function__Physical Health & Services,Function__Professional Development,Function__Recruitment,Function__Research & Accountability,Function__School Administration,Function__School Supervision,Function__Security & Safety,Function__Social & Emotional,Function__Special Population Program Management & Support,Function__Student Assignment,Function__Student Transportation,Function__Substitute Compensation,Function__Teacher Compensation,Function__Untracked Budget Set-Aside,Function__Utilities,Object_Type__Base Salary/Compensation,Object_Type__Benefits,Object_Type__Contracted Services,Object_Type__Equipment & Equipment Lease,Object_Type__NO_LABEL,Object_Type__Other Compensation/Stipend,Object_Type__Other Non-Compensation,Object_Type__Rent/Utilities,Object_Type__Substitute Compensation,Object_Type__Supplies/Materials,Object_Type__Travel & Conferences,Operating_Status__Non-Operating,\"Operating_Status__Operating, Not PreK-12\",Operating_Status__PreK-12 Operating,Position_Type__(Exec) Director,Position_Type__Area Officers,Position_Type__Club Advisor/Coach,Position_Type__Coordinator/Manager,Position_Type__Custodian,Position_Type__Guidance Counselor,Position_Type__Instructional Coach,Position_Type__Librarian,Position_Type__NO_LABEL,Position_Type__Non-Position,Position_Type__Nurse,Position_Type__Nurse Aide,Position_Type__Occupational Therapist,Position_Type__Other,Position_Type__Physical Therapist,Position_Type__Principal,Position_Type__Psychologist,Position_Type__School Monitor/Security,Position_Type__Sec/Clerk/Other Admin,Position_Type__Social Worker,Position_Type__Speech Therapist,Position_Type__Substitute,Position_Type__TA,Position_Type__Teacher,Position_Type__Vice Principal,Pre_K__NO_LABEL,Pre_K__Non PreK,Pre_K__PreK,Reporting__NO_LABEL,Reporting__Non-School,Reporting__School,Sharing__Leadership & Management,Sharing__NO_LABEL,Sharing__School Reported,Sharing__School on Central Budgets,Sharing__Shared Services,Student_Type__Alternative,Student_Type__At Risk,Student_Type__ELL,Student_Type__Gifted,Student_Type__NO_LABEL,Student_Type__Poverty,Student_Type__PreK,Student_Type__Special Education,Student_Type__Unspecified,Use__Business Services,Use__ISPD,Use__Instruction,Use__Leadership,Use__NO_LABEL,Use__O&M,Use__Pupil Services & Enrichment,Use__Untracked Budget Set-Aside'\n",
    "\n",
    "# write out weights\n",
    "print('writing weights to file')\n",
    "with open('weights.pkl', 'w') as f:\n",
    "    pickle.dump(w, f)\n",
    "\n",
    "output = './submission1234.csv'\n",
    "\n",
    "with open(output, 'w') as outfile:\n",
    "    outfile.write(h + '\\n')\n",
    "    for ID, x in data(test):\n",
    "        outfile.write(str(ID))\n",
    "        for k in K:\n",
    "            p = predict(x, w[k])\n",
    "            outfile.write(',%s' % str(p))\n",
    "        outfile.write('\\n')\n",
    "\n",
    "print('Done, elapsed time: %s' % str(datetime.now() - start))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
